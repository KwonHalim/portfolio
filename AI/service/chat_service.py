# Langchain ë©”ëª¨ë¦¬ ê´€ë ¨ í´ë˜ìŠ¤ë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from typing import Dict

from fastapi.logger import logger
from langchain_core.language_models import BaseLanguageModel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# ê¸°ì¡´ì— ì •ì˜í•œ Repositoryì™€ Retrieverë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
from database.chat.repository import ChatRepository
from database.vector.repository import VectorRepository
from service.langchain.document_retriever import DocumentRetriever


class ChatService:
    """
    Langchain ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ê³ ,
    ê²°ê³¼ë¥¼ Repositoryì— ì €ì¥í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
    """

    def __init__(
        self,
        retriever: DocumentRetriever,
        prompt: ChatPromptTemplate,
        llm: BaseLanguageModel,
        chat_repository: ChatRepository,
        vector_repository: VectorRepository,
    ):
        self.retriever = retriever
        self.prompt = prompt
        self.llm = llm
        self.chat_repository = chat_repository
        self.vector_repository = vector_repository
        print("âœ… ChatService ì´ˆê¸°í™” (Langchain ë©”ëª¨ë¦¬ ì‚¬ìš©)")

    def ask(self, question: str, session_id: str) -> Dict[str, str]:
        """
        ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ RAG ì²´ì¸ì„ ì‹¤í–‰í•˜ê³ , ê²°ê³¼ë¥¼ ì €ì¥í•œ í›„ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if len(question) > 200:
            return {"answer": "ì§ˆë¬¸ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. 200ì ì´í•˜ë¡œ ì¤„ì—¬ì£¼ì„¸ìš”.", "message_id": None}
        logger.info(f"--- ğŸ—£ï¸ ì§ˆë¬¸: {question} (Chat Session: {session_id}) ---")
        ###############ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘###############
        retriever_output = self.retriever.invoke(question)
        context = retriever_output["context"]
        ###############ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ###############
        source_docs = retriever_output["source_docs"]
        source_ids = [doc.metadata.get("source_id") for doc in source_docs if "source_id" in doc.metadata]
        chain = self.prompt | self.llm | StrOutputParser()
        ###############AI ì§ˆë¬¸###############
        answer = chain.invoke({
            "context": context,
            "question": question
        })

        # ì—¬ê¸° ë¶€ë¶„ì—ì„œ MongoDBì— ì›ë³¸ ì†ŒìŠ¤ë¥¼ ì–´ë–»ê²Œ ì €ì¥í• ì§€ ê²°ì •.
        # í˜„ì¬ëŠ” DBì˜ metadataì— ì¶œì²˜ retrieved_Source_idsë§Œ dic:List[str] í˜•íƒœë¡œ ì €ì¥
        metadata = {
            "retrieved_source_ids": source_ids
        }

        chat_id = self.chat_repository.save_chat(answer, question, session_id, metadata)

        return {"llm_answer" : answer,"chat_id": chat_id}

    def feedback(self, chat_id: str, is_good: bool) -> bool:
        """
        ì±„íŒ… ë©”ì‹œì§€ì— ëŒ€í•œ í”¼ë“œë°±ì„ ì €ì¥í•©ë‹ˆë‹¤.
        :param chat_repository: ì±„íŒ… ë ˆí¬ì§€í† ë¦¬ ì¸ìŠ¤í„´ìŠ¤
        :param chat_id: í”¼ë“œë°±ì„ ë‚¨ê¸¸ ì±„íŒ… ë©”ì‹œì§€ ID
        :param is_good: í”¼ë“œë°±ì´ ê¸ì •ì ì¸ì§€ ì—¬ë¶€ (True/False)
        """

        updated_chat_document = self.chat_repository.update_feedback(chat_id, is_good)
        source = updated_chat_document["metadata"]["retrieved_source_ids"]
        self.vector_repository.find_by_source_id(source_id=source, is_good = is_good)
        return True